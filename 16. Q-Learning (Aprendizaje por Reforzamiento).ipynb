{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21409b0d",
   "metadata": {},
   "source": [
    "# Q-Learning (Aprendizaje por Reforzamiento)\n",
    "\n",
    "Q-learning es un tipo de algoritmo de aprendizaje por refuerzo que permite a un agente aprender a tomar decisiones óptimas en un entorno específico.\n",
    "\n",
    "La \"Q\" en Q-learning se refiere a la calidad de las acciones que el agente puede tomar en cada estado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77285891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60754d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensiones = (5, 5) #25 posiciones posibles\n",
    "estado_inicial = (0, 0)\n",
    "estado_objetivo = (4, 4)\n",
    "obstaculos = [(1,1), (1,3), (2,3), (3,0)]\n",
    "acciones = [(-1,0), (1,0), (0,-1), (0,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6676b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_estados = dimensiones[0]*dimensiones[1]\n",
    "num_estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc8402ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_acciones =len(acciones)\n",
    "num_acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dccb4808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros((num_estados, num_acciones))\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be0b337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estado_a_indice(estado): \n",
    "    return estado[0] * dimensiones[1]+estado[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c6eeb93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ejemplo = estado_a_indice((1,0))\n",
    "ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3023972",
   "metadata": {},
   "source": [
    "- epsilon se usa para definir la probabilidad de que el agente tome una acción aleatoria en lugar de la mejor acción conocida según la tabla Q, facilitando la exploración del entorno.\n",
    "\n",
    "- alpha es el factor de tasa de aprendizaje que controla cuánto se actualiza el valor Q en cada paso del aprendizaje.\n",
    "\n",
    "- gamma es el factor de descuento que determina la importancia de las recompensas futuras en comparación con las recompensas inmediatas.\n",
    "\n",
    "- Un valor alto de epsilon (como 0.9) significa que hay una alta probabilidad de que el agente tome una acción aleatoria, promoviendo así la exploración.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "336f5c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1 #Cuanto se actualiza Q en cada aprendizaje, establecemos un valor bajo (aprendizaje mas lento pero mas seguro) \n",
    "gamma = 0.99 #Factor de descuento, importancia de las recompensas, cercano a 1 \n",
    "epsilon = 0.2 #Agente no repita las mismas decisiones y explore más alternativas\n",
    "episodios = 100 #Número de veces que se repite el episodio (>episodio > aprendizaje)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fbcabc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elegir_accion(estado):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(range(num_acciones))\n",
    "    else:\n",
    "        return np.argmax(Q[estado_a_indice(estado)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7127b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#la función aplicar_accion devuelve el estado original y una penalización de -100.\n",
    "\n",
    "def aplicar_accion(estado, accion_idx):\n",
    "    accion = acciones[accion_idx]\n",
    "    nuevo_estado = tuple(np.add(estado, accion) % dimensiones)\n",
    "    \n",
    "    if nuevo_estado in obstaculos or nuevo_estado == estado: \n",
    "        return estado, -100, False\n",
    "    if nuevo_estado == estado_objetivo: \n",
    "        return nuevo_estado, 100, True\n",
    "    return nuevo_estado, -1, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "361aaaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episodio in range(episodios):\n",
    "    estado = estado_inicial\n",
    "    terminado = False\n",
    "    \n",
    "    while not terminado:\n",
    "        idx_estado = estado_a_indice(estado)\n",
    "        accion_idx = elegir_accion(estado)\n",
    "        nuevo_estado, recompensa, terminado = aplicar_accion(estado, accion_idx)\n",
    "        idx_nuevo_estado = estado_a_indice(nuevo_estado)\n",
    "        \n",
    "        Q[idx_estado, accion_idx] = Q[idx_estado, accion_idx]+alpha+ (recompensa + gamma * np.max(Q[idx_nuevo_estado])-Q[idx_estado, accion_idx])\n",
    "        \n",
    "        estado = nuevo_estado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c2121b",
   "metadata": {},
   "source": [
    "La matriz politica contiene la mejor acción que el agente debe tomar en cada estado basado en el conocimiento adquirido durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cef92dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politica = np.zeros(dimensiones, dtype=int)\n",
    "politica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e9e62c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Política aprendida (0: arriba, 1: abajo, 2:izquierda, 3:derecha)\n",
      "[[0 0 3 0 0]\n",
      " [0 0 2 0 0]\n",
      " [0 2 0 0 0]\n",
      " [0 1 2 0 1]\n",
      " [2 2 3 0 0]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(dimensiones[0]):\n",
    "    for j in range(dimensiones[1]):\n",
    "        estado = (i,j)\n",
    "        idx_estado = estado_a_indice(estado)\n",
    "        mejor_accion = np.argmax(Q[idx_estado])\n",
    "        politica[i,j] = mejor_accion \n",
    "        \n",
    "print('Política aprendida (0: arriba, 1: abajo, 2:izquierda, 3:derecha)')\n",
    "print(politica) #Conocimiento por el entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfc7cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d3f06b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
