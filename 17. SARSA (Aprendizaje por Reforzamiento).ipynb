{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78a2dced",
   "metadata": {},
   "source": [
    "# SARSA (Estado-Acción-Recompensa-Estado-Acción)   (Aprendizaje por Reforzamiento)\n",
    "\n",
    "SARSA proviene de las siglas en inglés de la expresión \"State-Action-Reward-State-Action\", lo cual describe el proceso que sigue el algoritmo al actualizar los valores Q.\n",
    "\n",
    "La principal diferencia es que SARSA toma en cuenta tanto el estado actual como la acción actual para predecir la próxima acción y su recompensa, lo que lo hace un algoritmo on-policy.\n",
    "\n",
    "Alpha es la tasa de aprendizaje que ajusta la rapidez con la que el agente actualiza sus estimaciones de valor Q en respuesta a nueva información.\n",
    "\n",
    "Epsilon regula la probabilidad de que el agente elija una acción al azar, fomentando la exploración de nuevas acciones frente a la explotación del conocimiento adquirido.\n",
    "\n",
    "La función estado_a_indice(estado) toma un estado en dos dimensiones y lo convierte en un índice único para poder ubicarlo en la tabla Q.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "928643ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89f87eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensiones = (4,4)\n",
    "estado_inicial = (0,0)\n",
    "estado_objetivo = (3,3)\n",
    "acciones = [(0,-1),(0,1),(-1,0),(1,0)]\n",
    "acciones_simbolos = ['↑','↓','←','→']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65eef3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_estados = dimensiones[0] * dimensiones[1]\n",
    "num_estados #Número de estados en el entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7a716ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de acciones posibles: 4\n"
     ]
    }
   ],
   "source": [
    "num_acciones = len(acciones)\n",
    "print(f'Número de acciones posibles: {num_acciones}') #Número de acciones posibles(arriba, abajo, izq, der.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "025c1b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros((num_estados, num_acciones))\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ee11075",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1 #Tasa de Aprendizaje\n",
    "gamma = 0.99 #Factor de Descuento\n",
    "epsilon = 0.2 #Tasa de exploración\n",
    "episodios = 1000 #Mientras mas alto el valor mas aprende\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dcae1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estado_a_indice(estado):\n",
    "    return estado[0] * dimensiones[1] + estado[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "987381b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estado_a_indice((3,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcbc19ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elegir_accion(estado):\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        return random.randint(0, num_acciones -1)\n",
    "    else:\n",
    "        return np.argmax(Q[estado_a_indice(estado)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750a0789",
   "metadata": {},
   "source": [
    "np.add(estado, accion) suma la posición actual del agente con el cambio indicado por la acción para obtener la nueva posición.\n",
    "\n",
    "El operador % aplica límites \"atravesables\", permitiendo que la posición se envuelva al otro lado de la cuadrícula si se excede.(np.add(estado, accion) % np.array(dimensiones))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4575007e",
   "metadata": {},
   "source": [
    "En cada paso del bucle, se actualiza el valor de la matriz Q con la fórmula de actualización Q-learning basada en la recompensa recibida y las acciones futuras esperadas. (while not terminado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a38df06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_accion(estado, accion_idx):\n",
    "    accion = acciones[accion_idx]\n",
    "    nuevo_estado = tuple(np.add(estado, accion) % np.array(dimensiones))\n",
    "    \n",
    "    if nuevo_estado == estado_objetivo:\n",
    "        recompensa = 1\n",
    "    else:\n",
    "        recompensa = -1\n",
    "        \n",
    "    return nuevo_estado, recompensa, nuevo_estado == estado_objetivo\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efd257cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episodio in range(episodios):\n",
    "    estado = estado_inicial\n",
    "    accion_idx = elegir_accion(estado)\n",
    "    terminado = False\n",
    "    \n",
    "    while not terminado: \n",
    "        nuevo_estado, recompensa, terminado = aplicar_accion(estado, accion_idx)\n",
    "        nueva_accion_idx = elegir_accion(nuevo_estado)\n",
    "        \n",
    "        indice = estado_a_indice(estado)\n",
    "        Q[indice, accion_idx] += alpha * (recompensa + gamma * Q[estado_a_indice(nuevo_estado), nueva_accion_idx] - Q[indice, accion_idx])\n",
    "        \n",
    "        estado, accion_idx = nuevo_estado, nueva_accion_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "556ea47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['', '', '', ''],\n",
       "       ['', '', '', ''],\n",
       "       ['', '', '', ''],\n",
       "       ['', '', '', '']], dtype='<U2')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politica_simbolos = np.empty(dimensiones, dtype='<U2')\n",
    "politica_simbolos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f79283",
   "metadata": {},
   "source": [
    "politica_simbolos es utilizada para visualizar las mejores acciones en cada estado con símbolos como flechas.\n",
    "\n",
    "np.argmax(Q[estado_a_indice(estado)]) encuentra el índice de la acción con el mayor valor Q para un estado específico, indicando la mejor acción a tomar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79e4a3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['←', '→', '←', '←'],\n",
       "       ['↓', '←', '→', '→'],\n",
       "       ['→', '→', '→', '→'],\n",
       "       ['↑', '↓', '↓', '↑']], dtype='<U2')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(dimensiones[0]):\n",
    "    for j in range(dimensiones[1]):\n",
    "        estado = (i, j)\n",
    "        mejor_accion = np.argmax(Q[estado_a_indice(estado)])\n",
    "        politica_simbolos[i, j] = acciones_simbolos[mejor_accion]\n",
    "politica_simbolos        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aebd02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
